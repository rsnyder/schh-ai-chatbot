{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain RAG with chat history and streaming\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "We'll use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-community langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load API keys into environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct retriever and populate vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import bs4\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "  web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "  bs_kwargs=dict(\n",
    "    parse_only=bs4.SoupStrainer(class_=('post-content', 'post-title', 'post-header'))\n",
    "  )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Pinecone vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "\n",
    "model_name = 'text-embedding-ada-002'  \n",
    "index_name = 'schh-kb'\n",
    "text_field = 'text'\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "index = pc.Index(index_name)\n",
    "embeddings = OpenAIEmbeddings( model=model_name, openai_api_key=os.getenv('OPENAPI_API_KEY') )\n",
    "vectorstore = PineconeVectorStore( index, embeddings, text_field )  \n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextualize question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "contextualize_q_system_prompt = '''Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.'''\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "system_prompt = '''You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}'''\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    ('system', system_prompt),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{input}'),\n",
    "  ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statefully manage chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input',\n",
    "    history_messages_key='chat_history',\n",
    "    output_messages_key='answer',\n",
    ").with_config(tags=['main_chain'])\n",
    "\n",
    "def print_response(resp):\n",
    "  as_dict = {\n",
    "    'input': resp['input'],\n",
    "    'chat_history': [doc.model_dump() for doc in resp['chat_history']],\n",
    "    'context': [doc.model_dump() for doc in resp['context']],\n",
    "    'answer': resp['answer']\n",
    "  }\n",
    "  print(json.dumps(as_dict, indent=2) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "async def generate_chat_events(message, session_id):\n",
    "  \n",
    "  def serialize_aimessagechunk(chunk):\n",
    "    if isinstance(chunk, AIMessageChunk):\n",
    "      return chunk.content\n",
    "    else:\n",
    "      raise TypeError(f'Object of type {type(chunk).__name__} is not correctly formatted for serialization')\n",
    "  \n",
    "  try:\n",
    "    async for event in conversational_rag_chain.astream_events(message, version='v1', config={'configurable': {'session_id': session_id}} ):\n",
    "      # print(event['tags'], event['event'], event.get('data',{}).get('chunk'))\n",
    "      # Only get the answer\n",
    "      sources_tags = ['seq:step:3', 'main_chain']\n",
    "      if all(value in event['tags'] for value in sources_tags) and event['event'] == 'on_chat_model_stream':\n",
    "        chunk_content = serialize_aimessagechunk(event['data']['chunk'])\n",
    "        if len(chunk_content) != 0:\n",
    "          yield chunk_content\n",
    "          \n",
    "  except Exception as e:\n",
    "    print('error'+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three golf plans available: the 9-Hole Plan, the Standard Plan, and the Premium Plan. The 9-Hole Plan has a plan fee of $335 for singles and $568 for couples, with varying rates for morning and afternoon play. The Standard Plan costs $802 for singles and $1,364 for couples, while the Premium Plan offers unlimited buckets and costs $2,638 for singles and $4,481 for couples.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1 = 'tell me about the golf plans'\n",
    "sessionid = 'abc123'\n",
    "\n",
    "conversational_rag_chain.invoke(\n",
    "    {'input': prompt_1},\n",
    "    config={\n",
    "        'configurable': {'session_id': sessionid}\n",
    "    },\n",
    ")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, if an application is denied, homeowners can revise the application or appeal the decision by contacting the Modifications Coordinator listed in their denial letter for next steps.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_2 = 'Is there an appeal process for this?'\n",
    "\n",
    "conversational_rag_chain.invoke(\n",
    "    {'input': prompt_2},\n",
    "    config={'configurable': {'session_id': sessionid}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage with streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task\n",
      " decomposition\n",
      " is\n",
      " the\n",
      " process\n",
      " of\n",
      " breaking\n",
      " down\n",
      " a\n",
      " complex\n",
      " task\n",
      " into\n",
      " smaller\n",
      ",\n",
      " simpler\n",
      " steps\n",
      " to\n",
      " make\n",
      " it\n",
      " more\n",
      " manageable\n",
      ".\n",
      " This\n",
      " approach\n",
      " enhances\n",
      " model\n",
      " performance\n",
      " by\n",
      " allowing\n",
      " for\n",
      " structured\n",
      " reasoning\n",
      " and\n",
      " easier\n",
      " execution\n",
      " of\n",
      " tasks\n",
      ".\n",
      " Techniques\n",
      " such\n",
      " as\n",
      " Chain\n",
      " of\n",
      " Thought\n",
      " (\n",
      "Co\n",
      "T\n",
      ")\n",
      " and\n",
      " Tree\n",
      " of\n",
      " Thoughts\n",
      " are\n",
      " commonly\n",
      " used\n",
      " to\n",
      " facilitate\n",
      " this\n",
      " process\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "async for event in generate_chat_events({'input': prompt_1, 'chat_history': []}, sessionid):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse, Response, FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from typing import Optional\n",
    "import secrets\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware, allow_origins=['*'], allow_credentials=True, allow_methods=['*'], allow_headers=['*'])\n",
    "\n",
    "@app.get('/')\n",
    "async def root():\n",
    "  return FileResponse('index.html')\n",
    "\n",
    "@app.get('/chat/{prompt}')\n",
    "async def chat(prompt: str, sessionid: Optional[str] = None, stream: Optional[bool] = False):\n",
    "    sessionid = sessionid or secrets.token_hex(4) # Generates 4 bytes, resulting in an 8-character hex string\n",
    "\n",
    "    if stream:\n",
    "        return StreamingResponse(generate_chat_events({'input': prompt, 'chat_history': []}, sessionid), media_type='text/event-stream')\n",
    "    else:\n",
    "        resp = conversational_rag_chain.invoke({'input': prompt}, config={'configurable': {'session_id': sessionid}}, )\n",
    "        # print(json.dumps(store[sessionid].model_dump(), indent=2))\n",
    "        # print_response(resp)\n",
    "        return Response(status_code=200, content=resp['answer'], media_type='text/plain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
